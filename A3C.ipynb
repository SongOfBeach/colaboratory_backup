{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3C",
      "provenance": [],
      "collapsed_sections": [
        "oAaxo8EcssCI",
        "4qJyDTpFsUCO"
      ],
      "authorship_tag": "ABX9TyMBFFuWIZ/EqueoKVELY1DK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kldfznmsg/colaboratory_backup/blob/main/A3C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安装运行环境"
      ],
      "metadata": {
        "id": "fmPXI_Plp7Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 挂载Google云盘"
      ],
      "metadata": {
        "id": "LkBOi9hhtWFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Y61ChGwqTXJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03c3d52-11fe-42c7-b0c2-75c84151bc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 基础环境安装"
      ],
      "metadata": {
        "id": "fPqRnTUk40sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#选择TensorFlow版本\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "#安装游戏\n",
        "!pip install gym[atari] > /dev/null 2>&1 \n",
        "\n",
        "#安装环境依赖\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "#安装虚拟界面\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!python -m atari_py.import_roms /content/gdrive/MyDrive/prj/data/games"
      ],
      "metadata": {
        "id": "-xjntHp1owB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 下载Atari游戏包（仅第一次的时候运行）"
      ],
      "metadata": {
        "id": "0ijWsl1A47HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#安装atari游戏\n",
        "!pip install atari-py\n",
        "\n",
        "#下载解压、加载游戏列表\n",
        "!wget -P /content/gdrive/MyDrive/prj/data http://www.atarimania.com/roms/Roms.rar\n",
        "!unrar e /content/gdrive/MyDrive/prj/data/Roms.rar /content/gdrive/MyDrive/prj/data/games\n",
        "!python -m atari_py.import_roms /content/gdrive/MyDrive/prj/data/games"
      ],
      "metadata": {
        "id": "zKTp_hv408CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "KbtK1TSupvQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 在jupter里训练"
      ],
      "metadata": {
        "id": "8K64NC9TuJYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 加载Atari环境"
      ],
      "metadata": {
        "id": "oAaxo8EcssCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class AtariEnvironment(object):\n",
        "    \"\"\"\n",
        "    Small wrapper for gym atari environments.\n",
        "    Responsible for preprocessing screens and holding on to a screen buffer \n",
        "    of size agent_history_length from which environment state\n",
        "    is constructed.\n",
        "    \"\"\"\n",
        "    def __init__(self, gym_env, resized_width, resized_height, agent_history_length):\n",
        "        self.env = gym_env\n",
        "        self.resized_width = resized_width\n",
        "        self.resized_height = resized_height\n",
        "        self.agent_history_length = agent_history_length\n",
        "\n",
        "        self.gym_actions = range(gym_env.action_space.n)\n",
        "        if (gym_env.spec.id == \"Pong-v0\" or gym_env.spec.id == \"Breakout-v0\"):\n",
        "            print (\"Doing workaround for pong or breakout\")\n",
        "            # Gym returns 6 possible actions for breakout and pong.\n",
        "            # Only three are used, the rest are no-ops. This just lets us\n",
        "            # pick from a simplified \"LEFT\", \"RIGHT\", \"NOOP\" action space.\n",
        "            self.gym_actions = [1,2,3]\n",
        "\n",
        "        # Screen buffer of size AGENT_HISTORY_LENGTH to be able\n",
        "        # to build state arrays of size [1, AGENT_HISTORY_LENGTH, width, height]\n",
        "        self.state_buffer = deque()\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        \"\"\"\n",
        "        Resets the atari game, clears the state buffer\n",
        "        \"\"\"\n",
        "        # Clear the state buffer\n",
        "        self.state_buffer = deque()\n",
        "\n",
        "        x_t = self.env.reset()\n",
        "        x_t = self.get_preprocessed_frame(x_t)\n",
        "        s_t = np.stack((x_t, x_t, x_t, x_t), axis = 0)\n",
        "        \n",
        "        for i in range(self.agent_history_length-1):\n",
        "            self.state_buffer.append(x_t)\n",
        "        return s_t\n",
        "\n",
        "    def get_preprocessed_frame(self, observation):\n",
        "        \"\"\"\n",
        "        See Methods->Preprocessing in Mnih et al.\n",
        "        1) Get image grayscale\n",
        "        2) Rescale image\n",
        "        \"\"\"\n",
        "        return resize(rgb2gray(observation), (self.resized_width, self.resized_height))\n",
        "\n",
        "    def step(self, action_index):\n",
        "        \"\"\"\n",
        "        Excecutes an action in the gym environment.\n",
        "        Builds current state (concatenation of agent_history_length-1 previous frames and current one).\n",
        "        Pops oldest frame, adds current frame to the state buffer.\n",
        "        Returns current state.\n",
        "        \"\"\"\n",
        "\n",
        "        x_t1, r_t, terminal, info = self.env.step(self.gym_actions[action_index])\n",
        "        x_t1 = self.get_preprocessed_frame(x_t1)\n",
        "\n",
        "        previous_frames = np.array(self.state_buffer)\n",
        "        s_t1 = np.empty((self.agent_history_length, self.resized_height, self.resized_width))\n",
        "        s_t1[:self.agent_history_length-1, ...] = previous_frames\n",
        "        s_t1[self.agent_history_length-1] = x_t1\n",
        "\n",
        "        # Pop the oldest frame, add the current frame to the queue\n",
        "        self.state_buffer.popleft()\n",
        "        self.state_buffer.append(x_t1)\n",
        "\n",
        "        return s_t1, r_t, terminal, info\n"
      ],
      "metadata": {
        "id": "SJWfaOx8swdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###加载A3C模型"
      ],
      "metadata": {
        "id": "4qJyDTpFsUCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers import Convolution2D, Flatten, Dense, Input\n",
        "from keras.models import Model\n",
        "\n",
        "def build_policy_and_value_networks(num_actions, agent_history_length, resized_width, resized_height):\n",
        "    with tf.device(\"/cpu:0\"):\n",
        "        state = tf.placeholder(\"float\", [None, agent_history_length, resized_width, resized_height])\n",
        "\n",
        "        #共同训练的部分:输入层-卷积层1-卷积层2-Flatten-relu\n",
        "        inputs = Input(shape=(agent_history_length, resized_width, resized_height,))\n",
        "        shared = Convolution2D(name=\"conv1\", nb_filter=16, nb_row=8, nb_col=8, subsample=(4,4), activation='relu', border_mode='same')(inputs)\n",
        "        shared = Convolution2D(name=\"conv2\", nb_filter=32, nb_row=4, nb_col=4, subsample=(2,2), activation='relu', border_mode='same')(shared)\n",
        "        shared = Flatten()(shared)\n",
        "        shared = Dense(name=\"h1\", output_dim=256, activation='relu')(shared)\n",
        "\n",
        "        #训练actor\n",
        "        action_probs = Dense(name=\"p\", output_dim=num_actions, activation='softmax')(shared)\n",
        "\n",
        "        #训练critic\n",
        "        state_value = Dense(name=\"v\", output_dim=1, activation='linear')(shared)\n",
        "\n",
        "        policy_network = Model(input=inputs, output=action_probs)\n",
        "        value_network = Model(input=inputs, output=state_value)\n",
        "\n",
        "        p_params = policy_network.trainable_weights\n",
        "        v_params = value_network.trainable_weights\n",
        "\n",
        "        p_out = policy_network(state)\n",
        "        v_out = value_network(state)\n",
        "    print(\"a3c_model\")\n",
        "    return state, p_out, v_out, p_params, v_params"
      ],
      "metadata": {
        "id": "QYSEKCTYsSan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59d6812-3a2b-4609-939a-9aabc8ea9698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A3C主函数训练（a3c.py）"
      ],
      "metadata": {
        "id": "7QN4hqjcsJCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "加载相关函数"
      ],
      "metadata": {
        "id": "kuVnhjCGvyvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "import threading\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import gym\n",
        "from keras import backend as K\n",
        "from keras.layers import Convolution2D, Flatten, Dense\n",
        "from collections import deque\n",
        "#from a3c_model import build_policy_and_value_networks\n",
        "from keras import backend as K\n",
        "#from atari_environment import AtariEnvironment\n",
        "from pyvirtualdisplay import Display\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Path params\n",
        "EXPERIMENT_NAME = \"breakout_a3c\"\n",
        "PRJ_ROOT_DIR = \"/content/gdrive/MyDrive/prj/data\"\n",
        "SUMMARY_SAVE_PATH = PRJ_ROOT_DIR+\"/summaries/\"+EXPERIMENT_NAME\n",
        "CHECKPOINT_SAVE_PATH = \"/tmp/\"+EXPERIMENT_NAME+\".ckpt\"\n",
        "CHECKPOINT_NAME = CHECKPOINT_SAVE_PATH+\"-5\"\n",
        "CHECKPOINT_INTERVAL=5000\n",
        "SUMMARY_INTERVAL=5\n",
        "# TRAINING = False\n",
        "TRAINING = True\n",
        "\n",
        "#SHOW_TRAINING = True\n",
        "SHOW_TRAINING = False\n",
        "\n",
        "# Experiment params\n",
        "GAME = \"Breakout-v0\"\n",
        "ACTIONS = 3\n",
        "NUM_CONCURRENT = 8\n",
        "NUM_EPISODES = 20000\n",
        "\n",
        "AGENT_HISTORY_LENGTH = 4\n",
        "RESIZED_WIDTH = 84\n",
        "RESIZED_HEIGHT = 84\n",
        "\n",
        "# DQN Params\n",
        "GAMMA = 0.99\n",
        "\n",
        "# Optimization Params\n",
        "LEARNING_RATE = 0.00001\n",
        "\n",
        "#Shared global parameters\n",
        "T = 0\n",
        "TMAX = 80000000\n",
        "t_max = 32\n",
        "\n",
        "def start_pyvirtual_screen():\n",
        "  _display = Display(visible=0, size=(640,480))\n",
        "  _display.start()\n",
        "\n",
        "\n",
        "def render_env(env,mode='virtual'):\n",
        "  #渲染游戏界面\n",
        "  if mode=='local':\n",
        "    env.render()\n",
        "  elif mode=='virtual':\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "  else:\n",
        "    print(\"Param 'mode' in Function 'render_env' need be 'local'or'virtual'\")\n",
        "\n",
        "\n",
        "def sample_policy_action(num_actions, probs):\n",
        "    \"\"\"\n",
        "    Sample an action from an action probability distribution output by\n",
        "    the policy network.\n",
        "    \"\"\"\n",
        "    # Subtract a tiny value from probabilities in order to avoid\n",
        "    # \"ValueError: sum(pvals[:-1]) > 1.0\" in numpy.multinomial\n",
        "    probs = probs - np.finfo(np.float32).epsneg\n",
        "\n",
        "    histogram = np.random.multinomial(1, probs)\n",
        "    action_index = int(np.nonzero(histogram)[0])\n",
        "    return action_index\n",
        "\n",
        "def actor_learner_thread(num, env, session, graph_ops, summary_ops, saver):\n",
        "    # We use global shared counter T, and TMAX constant\n",
        "    global TMAX, T\n",
        "\n",
        "    # Unpack graph ops\n",
        "    s, a, R, minimize, p_network, v_network = graph_ops\n",
        "\n",
        "    # Unpack tensorboard summary stuff\n",
        "    r_summary_placeholder, update_ep_reward, val_summary_placeholder, update_ep_val, summary_op = summary_ops\n",
        "\n",
        "    # Wrap env with AtariEnvironment helper class\n",
        "    env_game=env\n",
        "    env = AtariEnvironment(gym_env=env, resized_width=RESIZED_WIDTH, resized_height=RESIZED_HEIGHT, agent_history_length=AGENT_HISTORY_LENGTH)\n",
        "\n",
        "    time.sleep(5*num)\n",
        "\n",
        "    # Set up per-episode counters\n",
        "    ep_reward = 0\n",
        "    ep_avg_v = 0\n",
        "    v_steps = 0\n",
        "    ep_t = 0\n",
        "\n",
        "    probs_summary_t = 0\n",
        "\n",
        "    s_t = env.get_initial_state()\n",
        "    terminal = False\n",
        "\n",
        "    while T < TMAX:\n",
        "        s_batch = []\n",
        "        past_rewards = []\n",
        "        a_batch = []\n",
        "\n",
        "        t = 0\n",
        "        t_start = t\n",
        "\n",
        "        while not (terminal or ((t - t_start)  == t_max)):\n",
        "            # Perform action a_t according to policy pi(a_t | s_t)\n",
        "            probs = session.run(p_network, feed_dict={s: [s_t]})[0]\n",
        "            action_index = sample_policy_action(ACTIONS, probs)\n",
        "            a_t = np.zeros([ACTIONS])\n",
        "            a_t[action_index] = 1\n",
        "\n",
        "            if probs_summary_t % 100 == 0:\n",
        "                print(\"THREAD:\", num,\"P, \", np.max(probs), \"V \", session.run(v_network, feed_dict={s: [s_t]})[0][0])\n",
        "                \n",
        "\n",
        "            s_batch.append(s_t)\n",
        "            a_batch.append(a_t)\n",
        "\n",
        "            s_t1, r_t, terminal, info = env.step(action_index)\n",
        "            ep_reward += r_t\n",
        "\n",
        "            r_t = np.clip(r_t, -1, 1)\n",
        "            past_rewards.append(r_t)\n",
        "\n",
        "            t += 1\n",
        "            T += 1\n",
        "            ep_t += 1\n",
        "            probs_summary_t += 1\n",
        "            \n",
        "            s_t = s_t1\n",
        "\n",
        "        if terminal:\n",
        "            R_t = 0\n",
        "        else:\n",
        "            R_t = session.run(v_network, feed_dict={s: [s_t]})[0][0] # Bootstrap from last state\n",
        "\n",
        "        R_batch = np.zeros(t)\n",
        "        for i in reversed(range(t_start, t)):\n",
        "            R_t = past_rewards[i] + GAMMA * R_t\n",
        "            R_batch[i] = R_t\n",
        "\n",
        "        session.run(minimize, feed_dict={R : R_batch,\n",
        "                                         a : a_batch,\n",
        "                                         s : s_batch})\n",
        "        \n",
        "        # Save progress every 5000 iterations\n",
        "        if T % CHECKPOINT_INTERVAL == 0:\n",
        "            saver.save(session, CHECKPOINT_SAVE_PATH, global_step = T)\n",
        "\n",
        "        if terminal:\n",
        "            # Episode ended, collect stats and reset game\n",
        "            session.run(update_ep_reward, feed_dict={r_summary_placeholder: ep_reward})\n",
        "            print(\"THREAD:\", num, \"/ TIME\", T, \"/ REWARD\", ep_reward)\n",
        "            s_t = env.get_initial_state()\n",
        "            terminal = False\n",
        "            # Reset per-episode counters\n",
        "            ep_reward = 0\n",
        "            ep_t = 0\n",
        "\n",
        "def build_graph():\n",
        "    # Create shared global policy and value networks\n",
        "    s, p_network, v_network, p_params, v_params = build_policy_and_value_networks(num_actions=ACTIONS, agent_history_length=AGENT_HISTORY_LENGTH, resized_width=RESIZED_WIDTH, resized_height=RESIZED_HEIGHT)\n",
        "\n",
        "    # Shared global optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
        "\n",
        "    # Op for applying remote gradients\n",
        "    R_t = tf.placeholder(\"float\", [None])\n",
        "    a_t = tf.placeholder(\"float\", [None, ACTIONS])\n",
        "    log_prob = tf.log(tf.reduce_sum(p_network * a_t, reduction_indices=1))\n",
        "    p_loss = -log_prob * (R_t - v_network)\n",
        "    v_loss = tf.reduce_mean(tf.square(R_t - v_network))\n",
        "\n",
        "    total_loss = p_loss + (0.5 * v_loss)\n",
        "\n",
        "    minimize = optimizer.minimize(total_loss)\n",
        "    return s, a_t, R_t, minimize, p_network, v_network\n",
        "\n",
        "# Set up some episode summary ops to visualize on tensorboard.\n",
        "def setup_summaries():\n",
        "    episode_reward = tf.Variable(0.)\n",
        "    tf.summary.scalar(\"Episode Reward\", episode_reward)\n",
        "    r_summary_placeholder = tf.placeholder(\"float\")\n",
        "    update_ep_reward = episode_reward.assign(r_summary_placeholder)\n",
        "    ep_avg_v = tf.Variable(0.)\n",
        "    tf.summary.scalar(\"Episode Value\", ep_avg_v)\n",
        "    val_summary_placeholder = tf.placeholder(\"float\")\n",
        "    update_ep_val = ep_avg_v.assign(val_summary_placeholder)\n",
        "    summary_op = tf.summary.merge_all()\n",
        "    return r_summary_placeholder, update_ep_reward, val_summary_placeholder, update_ep_val, summary_op\n",
        "\n",
        "def train(session, graph_ops, saver):\n",
        "    # Set up game environments (one per thread)\n",
        "    envs = [gym.make(GAME) for i in range(NUM_CONCURRENT)]\n",
        "    \n",
        "    summary_ops = setup_summaries()\n",
        "    summary_op = summary_ops[-1]\n",
        "\n",
        "    # Initialize variables\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    writer = tf.summary.FileWriter(SUMMARY_SAVE_PATH, session.graph)\n",
        "\n",
        "    # Start NUM_CONCURRENT training threads\n",
        "    actor_learner_threads = [threading.Thread(target=actor_learner_thread, args=(thread_id, envs[thread_id], session, graph_ops, summary_ops, saver)) for thread_id in range(NUM_CONCURRENT)]\n",
        "    for t in actor_learner_threads:\n",
        "        t.start()\n",
        "\n",
        "    # Show the agents training and write summary statistics\n",
        "    last_summary_time = 0\n",
        "    while True:\n",
        "        if SHOW_TRAINING:\n",
        "            for env in envs:\n",
        "                render_env(env);\n",
        "        now = time.time()\n",
        "        if now - last_summary_time > SUMMARY_INTERVAL:\n",
        "            summary_str = session.run(summary_op)\n",
        "            writer.add_summary(summary_str, float(T))\n",
        "            last_summary_time = now\n",
        "    for t in actor_learner_threads:\n",
        "        t.join()\n",
        "\n",
        "def evaluation(session, graph_ops, saver):\n",
        "    saver.restore(session, CHECKPOINT_NAME)\n",
        "    print(\"Restored model weights from \", CHECKPOINT_NAME)\n",
        "    monitor_env = gym.make(GAME)\n",
        "    monitor_env.monitor.start('/tmp/'+EXPERIMENT_NAME+\"/eval\")\n",
        "\n",
        "    # Unpack graph ops\n",
        "    s, a_t, R_t, minimize, p_network, v_network = graph_ops\n",
        "\n",
        "    # Wrap env with AtariEnvironment helper class\n",
        "    env = AtariEnvironment(gym_env=monitor_env, resized_width=RESIZED_WIDTH, resized_height=RESIZED_HEIGHT, agent_history_length=AGENT_HISTORY_LENGTH)\n",
        "\n",
        "    for i_episode in xrange(100):\n",
        "        s_t = env.get_initial_state()\n",
        "        ep_reward = 0\n",
        "        terminal = False\n",
        "        while not terminal:\n",
        "            monitor_env.render()\n",
        "            # Forward the deep q network, get Q(s,a) values\n",
        "            probs = p_network.eval(session = session, feed_dict = {s : [s_t]})[0]\n",
        "            action_index = sample_policy_action(ACTIONS, probs)\n",
        "            s_t1, r_t, terminal, info = env.step(action_index)\n",
        "            s_t = s_t1\n",
        "            ep_reward += r_t\n",
        "        print( ep_reward)\n",
        "    monitor_env.monitor.close()\n"
      ],
      "metadata": {
        "id": "vD_8mPrhr_T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "训练"
      ],
      "metadata": {
        "id": "3hdWvomyv3AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(_):\n",
        "  start_pyvirtual_screen(); #启动虚拟界面，colab无图形化，用matplotlib实现\n",
        "  g = tf.Graph() #计算图，主要用于构建网络，本身不进行任何实际的计算。\n",
        "  with g.as_default(), tf.Session() as session: #开始会画执行定义的操作\n",
        "    K.set_session(session)\n",
        "    graph_ops = build_graph()\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    if TRAINING:\n",
        "        train(session, graph_ops, saver)\n",
        "    else:\n",
        "        evaluation(session, graph_ops, saver)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  tf.app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8-PKrO8vqej",
        "outputId": "ff636174-79ac-4fc0-cedd-0a7e1e8322b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"conv1\", activation=\"relu\", kernel_size=(8, 8), filters=16, strides=(4, 4), padding=\"same\")`\n",
            "  if sys.path[0] == '':\n",
            "W0411 19:23:36.427436 139742985955200 deprecation.py:506] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"conv2\", activation=\"relu\", kernel_size=(4, 4), filters=32, strides=(2, 2), padding=\"same\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(name=\"h1\", activation=\"relu\", units=256)`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(name=\"p\", activation=\"softmax\", units=3)`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(name=\"v\", activation=\"linear\", units=1)`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"p/...)`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"v/...)`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a3c_model\n",
            "INFO:tensorflow:Summary name Episode Reward is illegal; using Episode_Reward instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0411 19:23:38.301537 139742985955200 summary_op_util.py:66] Summary name Episode Reward is illegal; using Episode_Reward instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Summary name Episode Value is illegal; using Episode_Value instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I0411 19:23:38.312760 139742985955200 summary_op_util.py:66] Summary name Episode Value is illegal; using Episode_Value instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doing workaround for pong or breakout\n",
            "Doing workaround for pong or breakout\n",
            "Doing workaround for pong or breakout\n",
            "Doing workaround for pong or breakout\n",
            "Doing workaround for pong or breakout\n",
            "Doing workaround for pong or breakout\n",
            "Doing workaround for pong or breakout\n",
            "Doing workaround for pong or breakout\n",
            "THREAD: 0 P,  0.34499517 V  -0.025435545\n",
            "THREAD: 0 P,  0.3477712 V  -0.009897483\n",
            "THREAD: 0 P,  0.34853387 V  0.0051855575\n",
            "THREAD: 1 P,  0.34774157 V  0.0018332974\n",
            "THREAD: 0 P,  0.34800065 V  0.023341568\n",
            "THREAD: 1 P,  0.3505816 V  0.026907165\n",
            "THREAD: 0 / TIME 493 / REWARD 3.0\n",
            "THREAD: 0 P,  0.35295513 V  0.042321336\n",
            "THREAD: 2 P,  0.35265425 V  0.04531278\n",
            "THREAD: 1 P,  0.35465932 V  0.045340646\n",
            "THREAD: 1 / TIME 779 / REWARD 1.0\n",
            "THREAD: 0 P,  0.3547118 V  0.07079646\n",
            "THREAD: 2 P,  0.35475692 V  0.0723781\n",
            "THREAD: 3 P,  0.35494345 V  0.07469487\n",
            "THREAD: 0 / TIME 949 / REWARD 0.0\n",
            "THREAD: 1 P,  0.35669133 V  0.07839729\n",
            "THREAD: 2 / TIME 1168 / REWARD 0.0\n",
            "THREAD: 2 P,  0.35875428 V  0.11202829\n",
            "THREAD: 0 P,  0.35839936 V  0.1099459\n",
            "THREAD: 3 P,  0.35820007 V  0.11783725\n",
            "THREAD: 4 P,  0.358951 V  0.11895846\n",
            "THREAD: 1 P,  0.36023474 V  0.12076486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 直接在工程里训练"
      ],
      "metadata": {
        "id": "Rg4lJBlht4EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#训练a3c模型\n",
        "!python /content/drive/MyDrive/prj/a3c.py --experiment breakout --game \"Breakout-v0\" --num_concurrent 8"
      ],
      "metadata": {
        "id": "J36nRO_JfWe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#训练异步dqn模型\n",
        "!python /content/drive/MyDrive/prj/async_dqn.py --experiment breakout --game \"Breakout-v0\" --num_concurrent 8"
      ],
      "metadata": {
        "id": "nph5ZpkPOu70"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}